{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tokenization ===\n",
      "TEXT: The             LEMMA: the             HEAD: fox MORPH: Definite=Def|PronType=Art\n",
      "TEXT: quick           LEMMA: quick           HEAD: fox MORPH: Degree=Pos\n",
      "TEXT: brown           LEMMA: brown           HEAD: fox MORPH: Degree=Pos\n",
      "TEXT: fox             LEMMA: fox             HEAD: jump MORPH: Number=Sing\n",
      "TEXT: does            LEMMA: do              HEAD: jump MORPH: Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "TEXT: n't             LEMMA: not             HEAD: jump MORPH: Polarity=Neg\n",
      "TEXT: jump            LEMMA: jump            HEAD: jump MORPH: VerbForm=Inf\n",
      "TEXT: over            LEMMA: over            HEAD: jump MORPH: \n",
      "TEXT: the             LEMMA: the             HEAD: dog MORPH: Definite=Def|PronType=Art\n",
      "TEXT: lazy            LEMMA: lazy            HEAD: dog MORPH: Degree=Pos\n",
      "TEXT: dog             LEMMA: dog             HEAD: over MORPH: Number=Sing\n",
      "TEXT: .               LEMMA: .               HEAD: jump MORPH: PunctType=Peri\n",
      "TEXT: Natural         LEMMA: Natural         HEAD: Language MORPH: Number=Sing\n",
      "TEXT: Language        LEMMA: Language        HEAD: Processing MORPH: Number=Sing\n",
      "TEXT: Processing      LEMMA: processing      HEAD: is MORPH: Number=Sing\n",
      "TEXT: is              LEMMA: be              HEAD: is MORPH: Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "TEXT: fascinating     LEMMA: fascinating     HEAD: is MORPH: Degree=Pos\n",
      "TEXT: !               LEMMA: !               HEAD: is MORPH: PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample Text\n",
    "text = \"The quick brown fox doesn't jump over the lazy dog. Natural Language Processing is fascinating!\"\n",
    "\n",
    "# Process the text with spacy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Run the loop using text, lemma, head, and morph functions\n",
    "print(\"=== Tokenization ===\")\n",
    "for token in doc:\n",
    "    print(f'''TEXT: {token.text:<15} LEMMA: {token.lemma_:<15} HEAD: {token.head} MORPH: {token.morph}''')\n",
    "\n",
    "# Answers to Questions\n",
    "\n",
    "# 1) - How does spaCy process the various tokens?\n",
    "# spaCy treats each word, punctuation mark, and contracted word as a token. \n",
    "\n",
    "# 2) - How does spaCy handle punctuation marks like periods and commas?\n",
    "# Punctuation are treated as separate tokens. You can see them see them printed as indiviudal tokens with their own featuers\n",
    "\n",
    "# 3) - What happens when the text includes contractions?\n",
    "# spaCy keeps contractions like \"don't\" as a single token (it does not split \"do\" and \"not\"). The lemma is often expanded into (\"do not\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Part-of-Speech Tagging ===\n",
      "Token: The             POS: DET        TAG: DT\n",
      "Token: quick           POS: ADJ        TAG: JJ\n",
      "Token: brown           POS: ADJ        TAG: JJ\n",
      "Token: fox             POS: NOUN       TAG: NN\n",
      "Token: does            POS: AUX        TAG: VBZ\n",
      "Token: n't             POS: PART       TAG: RB\n",
      "Token: jump            POS: VERB       TAG: VB\n",
      "Token: over            POS: ADP        TAG: IN\n",
      "Token: the             POS: DET        TAG: DT\n",
      "Token: lazy            POS: ADJ        TAG: JJ\n",
      "Token: dog             POS: NOUN       TAG: NN\n",
      "Token: .               POS: PUNCT      TAG: .\n",
      "Token: Natural         POS: PROPN      TAG: NNP\n",
      "Token: Language        POS: PROPN      TAG: NNP\n",
      "Token: Processing      POS: NOUN       TAG: NN\n",
      "Token: is              POS: AUX        TAG: VBZ\n",
      "Token: fascinating     POS: ADJ        TAG: JJ\n",
      "Token: !               POS: PUNCT      TAG: .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the loop using pos and tag functions\n",
    "print(\"=== Part-of-Speech Tagging ===\")\n",
    "for token in doc:\n",
    "    print(f'Token: {token.text:<15} POS: {token.pos_:<10} TAG: {token.tag_}')\n",
    "\n",
    "# Answers to Questions\n",
    "\n",
    "# 1) - Identify the POS tags for \"quick\", \"jumps\", and \"is\"\n",
    "\n",
    "# \"quick\" -> ADJ(adjective)\n",
    "# \"jump\" -> VERB(verb)\n",
    "# \"is\" -> AUX(auxiliary verb)\n",
    "\n",
    "# 2) - Why might POS taggin be useful for tasks like grammar checking or machine translation?\n",
    "\n",
    "# POS tagging helps identify the grammatical role of each word in a sentence\n",
    "# For grammar checking, it allows us to detect incorrect usage of words\n",
    "# For machine translation, it hlpes preserve sentence structure and meaning across languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Named Entity Recognition (NER) ====\n",
      "ENTITY: Barack Obama              LABEL: PERSON\n",
      "ENTITY: 44th                      LABEL: ORDINAL\n",
      "ENTITY: the United States         LABEL: GPE\n",
      "ENTITY: Hawaii                    LABEL: GPE\n"
     ]
    }
   ],
   "source": [
    "# New text to analyze\n",
    "ner_text = \"Barack Obama was the 44th President of the United States. He was born in Hawaii.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(ner_text)\n",
    "\n",
    "# Run the loop using label function\n",
    "print('=== Named Entity Recognition (NER) ====')\n",
    "for ent in doc.ents:\n",
    "    print(f'ENTITY: {ent.text:<25} LABEL: {ent.label_}')\n",
    "\n",
    "\n",
    "# Answers to Questions\n",
    "\n",
    "# 1) - Which enetities are recognized by spaCy?\n",
    "\n",
    "# \"Barack Obama\", \"44th\", \"the United States\", \"Hawaii\"\n",
    "\n",
    "# 2) - What entity types are assigned to \"Barack Obama\" and \"Hawaii\"?\n",
    "\n",
    "# \"Barack Obama\" -> PERSON\n",
    "# \"Hawaii\" -> GPE (Geo-Political Entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tokens and POS tags ===\n",
      "I               POS: PRON       TAG: PRP\n",
      "believe         POS: VERB       TAG: VBP\n",
      "Shai            POS: PROPN      TAG: NNP\n",
      "Gigeous         POS: PROPN      TAG: NNP\n",
      "-               POS: PUNCT      TAG: HYPH\n",
      "Alander         POS: PROPN      TAG: NNP\n",
      "of              POS: ADP        TAG: IN\n",
      "the             POS: DET        TAG: DT\n",
      "OKC             POS: PROPN      TAG: NNP\n",
      "is              POS: AUX        TAG: VBZ\n",
      "going           POS: VERB       TAG: VBG\n",
      "to              POS: PART       TAG: TO\n",
      "win             POS: VERB       TAG: VB\n",
      "MVP             POS: PROPN      TAG: NNP\n",
      "over            POS: ADP        TAG: IN\n",
      "Nikola          POS: PROPN      TAG: NNP\n",
      "Jokic           POS: PROPN      TAG: NNP\n",
      "of              POS: ADP        TAG: IN\n",
      "the             POS: DET        TAG: DT\n",
      "NBA             POS: PROPN      TAG: NNP\n",
      "!               POS: PUNCT      TAG: .\n",
      "\n",
      "\n",
      "=== Named Entities ===\n",
      "Shai Gigeous-Alander      LABEL: PERSON\n",
      "OKC                       LABEL: ORG\n",
      "MVP                       LABEL: ORG\n",
      "Nikola Jokic              LABEL: ORG\n",
      "NBA                       LABEL: ORG\n"
     ]
    }
   ],
   "source": [
    "# My original sentence\n",
    "custom_text = \"I believe Shai Gigeous-Alander of the OKC is going to win MVP over Nikola Jokic of the NBA!\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(custom_text)\n",
    "\n",
    "# Running the pipeline, POS tagging\n",
    "print('=== Tokens and POS tags ===')\n",
    "for token in doc:\n",
    "    print(f'{token.text:<15} POS: {token.pos_:<10} TAG: {token.tag_}')\n",
    "\n",
    "# Named Entity Recognition\n",
    "print('\\n\\n=== Named Entities ===')\n",
    "for ent in doc.ents:\n",
    "    print(f'{ent.text:<25} LABEL: {ent.label_}')\n",
    "\n",
    "# Answers to Questions\n",
    "\n",
    "# spaCy recgonized Shai as a PERSON, which I wasn't sure it would be capable of doing. \n",
    "# Even when I drastically mispelled his name it recognized him as a person.\n",
    "# Recognized NBA as an ORG which is correct, but also identified MVP as an ORG which it isn't\n",
    "# Additionally, it regonized OKC, the abbreviated version of the Oklahoma City Thunder which surprised me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
